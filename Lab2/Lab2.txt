(lab6p6)
2.1: ...

(----)
2.2: 64 KB
cache aquecida para arrays < 64KB causa hits
               para arrays > 64KB é irrelevante pois vai conter os 
                              ultimos blocos do array
2.3: 16 B
stride = 4 => miss, hit hit hit 
stride = 8 => miss,hit             :> tempo de acesso vai aumentando
stride = 16 => miss                   até estabilizar (sempre miss)
etc
2.4: 1000 - 375 = 625 ns
Em 375 é o hit time pois a cache está aquecida e todo o array cabe
Em 1000 é o miss time pois está constantemente a falhar 

3  (lab6p4)
3.1
3.1.1 
a)  The "L1 data cache misses" event
Every time there is a miss in the L1 data cache, an event will be triggered

b) --

c)

 . Cache Size = 32KB
Until 32KB the average misses are negligible so we can assume the cache size is big enough to house 
the whole array, the small average misses showned in the graph can be justified by the cache being used
by another process and as a result not being able to fit the whole array
From 32KB and above, the average miss rate goes up significatly because the cache cant fit the entire
array resulting in multiple cache misses. 

 . Block Size = 64B
When using strides under 64B, the miss rate is lower because we are able to get at least a hit before
we enter a new block (the first access is always a miss). From 64B onwards, we only access each cache block
up to one time, resulting in a miss rate of 100% and, as a result, a stabilisation in the number of misses.


 . Associativity set size = 
The Associativity level can be found when analyzing higher strides. With the increase of strides, 
the number of access lowers, filling less and less lines. In the limit, a certain stride will only 
fill one line and the cache will only store as much as the level of Associativity.
If the number of access is higher than the Associativity level then the values first stored will be rewritten
by the newer ones, which will cause misses in latter accesses.
When we reach a point when all blocks fetched equal the level of Associativity, none of those blocks
will be replaced and the next stride will benefit from this blocks in cache, having zero misses.
In our graph this happens with an array = 64KB and stride = 8192 meaning that all blocks needed 
for this accesses are present in cache in the same set of Associativity. So 64KB/8KB = 8 access
and therefore 8 ways of Associativity.


3.1.2
a)
Change parameters of function PAPI_add_event to add l2 data cache misses to our event set.
This will able us to track L2 cache and latter analyze the characteristics of
the L2 cache. 
Change default values of CACHE_MIN and CACHE_MAX to 64KB and 1024KB to trigger the L2 cache

b)

3.2.1

a) The program described above is implemented in the multiply_matrices function in mm1.c source code.
Each matrix is size N*N where N=512 
each entry of the matrix is of size(int16_t) = 16 bits = 2B
Therefore the memory required to accomodate each matrix is 512*512*2B = 512KB

b)
After resetting counter 'PAPI_L1_DCM' [x10^6]: 0.000000
After resetting counter 'PAPI_LD_INS' [x10^6]: 0.000000
After resetting counter 'PAPI_SR_INS' [x10^6]: 0.000000
After stopping counter 'PAPI_L1_DCM'  [x10^6]: 134.346845
After stopping counter 'PAPI_LD_INS'  [x10^6]: 3491.008470
After stopping counter 'PAPI_SR_INS'  [x10^6]: 672.141375
Wall clock cycles [x10^6]: 4000.497626
Wall clock time [seconds]: 1.179302
Matrix checksum: 2717908992



Total number of L1 data cache misses 			134.3468
Total number of load / store instructions completed 	3491+672
Total number of clock cycles 				4000
Elapsed time						1.1793



c) 
Hit-Rate = 1 - 134.346845/(3491.008470+672.141375) = 0.9677

3.2.2

a)
The program described above is implemented in the transpose function and multiply_matrices function  in mm2.c source code.

After resetting counter 'PAPI_L1_DCM' [x10^6]: 0.000000
After resetting counter 'PAPI_LD_INS' [x10^6]: 0.000000
After resetting counter 'PAPI_SR_INS' [x10^6]: 0.000000
After stopping counter 'PAPI_L1_DCM'  [x10^6]: 4.211259
After stopping counter 'PAPI_LD_INS'  [x10^6]: 402.660006
After stopping counter 'PAPI_SR_INS'  [x10^6]: 134.217780
Wall clock cycles [x10^6]: 743.958066
Wall clock time [seconds]: 0.219311
Matrix checksum: 2717908992

Total number of L1 data cache misses 			4.21126 
Total number of load / store instructions completed	402.6600+134.2179
Total number of clock cycles 				743.9581
Elapsed time						0.2193	

b)
Hit-Rate = 1 - 4.211259/(402.660006+134.217780) = 0.9922

	

c)
After resetting counter 'PAPI_L1_DCM' [x10^6]: 0.000000
After resetting counter 'PAPI_LD_INS' [x10^6]: 0.000000
After resetting counter 'PAPI_SR_INS' [x10^6]: 0.000000
After stopping counter 'PAPI_L1_DCM'  [x10^6]: 4.483849
After stopping counter 'PAPI_LD_INS'  [x10^6]: 402.921948
After stopping counter 'PAPI_SR_INS'  [x10^6]: 134.479925
Wall clock cycles [x10^6]: 744.407653
Wall clock time [seconds]: 0.219443
Matrix checksum: 2717908992

Total number of L1 data cache misses 			4.4838
Total number of load / store instructions completed	402.9219+134.4799
Total number of clock cycles 				744.4076
Elapsed time						0.21944	

Because of the time complexitty associated with transposing a matrix O(N^2) when compared to the multiplication of matrices O(N^3), it does not introduce relevant time penalty in the overall clock time of the program.


d)
# mm2 a contar com a transposicao ou nao ? 

∆HitRate = HitRate1 - HitRate2 = 0.9922 - 0.9677 = 0.0245
Speedup = 4000.497626/743.958066 = 5.3773
Speedup = 1.179302/0.219311 = 5.3773
Comment:
Transpor é fixe


3.2.3

a)
The program described above matches with the function "multiply_matrices_by_blocks" in the source code.
From Section 3.1 the CLS (CACHE_LINE_SIZE) is 64B
Each matrix element is of size(int16_t) = 16 bits = 2B
Therefore we can accommodate 64/2 = 32 matrix elements in each cache line.
Alteracao: #define CACHE_LINE_SIZE 64 // TODO: update this value

b)
After resetting counter 'PAPI_L1_DCM' [x10^6]: 0.000000
After resetting counter 'PAPI_LD_INS' [x10^6]: 0.000000
After resetting counter 'PAPI_SR_INS' [x10^6]: 0.000000
After stopping counter 'PAPI_L1_DCM'  [x10^6]: 5.836904
After stopping counter 'PAPI_LD_INS'  [x10^6]: 402.801665
After stopping counter 'PAPI_SR_INS'  [x10^6]: 134.222203
Wall clock cycles [x10^6]: 401.652163
Wall clock time [seconds]: 0.118402
Matrix checksum: 2717908992

Total number of L1 data cache misses			5.836904
Total number of load / store instructions completed  	402.801665+134.222203
Total number of clock cycles 				401.652163
Elapsed time						0.118402

c)
(susy ? menor hit rate mas mais rapido)
Hit-rate:  1 - 5.836904/(402.801665+134.222203) = 0.9891

d) hit-rate (straightforward implementation) = 0.9677

(∆HitRate) = hit(straightforward) - hit(novo) = 0.9891 - 0.9677 = 0.0214

speedup = t(straightforward) - t(new) = 1.179302 - 0.118402 = 1.0609


e)

(∆HitRate) = hit(transpose) - hit(novo) = 0.9922 - 0.9677 = 0.0245

speedup = t(transpose) - t(new) = 0.21944 - 0.118402 = 0.101038